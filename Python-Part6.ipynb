{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python intensive, part 6\n",
    "\n",
    "## Putting it all together: exploring storm data\n",
    "\n",
    "For our final section, we are going to do some exploratory analysis of a real world dataset, coding in pairs or groups as we did on the previous days. There are several different \"paths\" you can choose to follow based on what you are interested in working on, so either come to a consensus within your group, or group up based on shared interest! First, let's download our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line downloads the file locally to the same folder as your notebook\n",
    "!wget https://raw.githubusercontent.com/harvardinformatics/python-intensive/refs/heads/main/data/indiana_storms_full.csv\n",
    "\n",
    "# This line stores the local file path as a Python string variable\n",
    "storms_file = 'indiana_storms_full.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "storms_df = pd.read_csv(storms_file)\n",
    "storms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: subsetting columns\n",
    "\n",
    "This is a file that contains data from the National Weather Service. It contains data on storm events in the state of Indiana in 2015. There is quite a lot of information in there with 50 columns in the dataset, so before we do any analysis let's do a bit of cleanup! \n",
    "\n",
    "We are primarily interested in type of storm event \"EVENT_TYPE\", the county in which the event occurred \"CZ_NAME\", and the time of the event \"BEGIN_DATE_TIME\", \"END_DATE_TIME\", \"CZ_TIMEZONE\". Let's also throw in there \"BEGIN_LOCATION\", \"END_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\", \"END_LAT\", and \"END_LON\" just in case. So the first thing we need to do is subset the data to only include these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df = storms_df[[\"EVENT_TYPE\", \"CZ_NAME\", \"BEGIN_DATE_TIME\", \"END_DATE_TIME\", \"CZ_TIMEZONE\",  \"BEGIN_LOCATION\", \"END_LOCATION\", \"BEGIN_LAT\", \"BEGIN_LON\", \"END_LAT\", \"END_LON\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check if there are any missing values in the columns we are interested in. If there are, we should remove the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only some storm events have a begin and end location, but at least there aren't any that only have a begin and not and end. Data looks good! The next thing we need to do is to convert the \"BEGIN_DATE_TIME\" and \"END_DATE_TIME\" columns to datetime objects. Additionally, these data are spread out over two time zones and we want to convert it to a single time zone for clarity. This will make it easier to work with these columns later on. For more information on the proper way to manage dates and times in Python, see the section in our Python Healthy Habits document \"Working with dates and times\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all date time columns to datetime objects with correct timezone parsing\n",
    "# I wrote a regex to extract just the number from the timezone column and then made it an ISO8601 compliant timezone string\n",
    "# Pandas does not support multiple time zones in the same column, so we will convert all times to UTC and then convert to one of the local time zones\n",
    "storms_df['BEGIN_DATE_TIME'] = pd.to_datetime(storms_df[\"BEGIN_DATE_TIME\"]+\"-0\"+storms_df[\"CZ_TIMEZONE\"].str.extract(r'([0-9]+)')[0]+\"00\", format=\"mixed\", utc=True).dt.tz_convert(\"-05:00\")\n",
    "storms_df['END_DATE_TIME'] = pd.to_datetime(storms_df[\"END_DATE_TIME\"]+\"-0\"+storms_df[\"CZ_TIMEZONE\"].str.extract(r'([0-9]+)')[0]+\"00\", format=\"mixed\", utc=True).dt.tz_convert(\"-05:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of the code I used to convert the \"BEGIN_DATE_TIME\" and \"END_DATE_TIME\" columns to datetime objects:\n",
    "\n",
    "```python\n",
    "storms_df['BEGIN_DATE_TIME'] = pd.to_datetime(storms_df[\"BEGIN_DATE_TIME\"]+\"-0\"+storms_df[\"CZ_TIMEZONE\"].str.extract(r'([0-9]+)')[0]+\"00\", format=\"mixed\", utc=True).dt.tz_convert(\"-05:00\")\n",
    "```\n",
    "\n",
    "|Code|Explanation|\n",
    "|---|---|\n",
    "|`pd.to_datetime()`|This function converts a string or a list of strings to a datetime object. However, I first need to convert the string into something that is can parse.|\n",
    "|`storms_df[\"BEGIN_DATE_TIME\"]`| This column is alredy in a format that pandas can parse, but it is missing the time zone information.|\n",
    "|`+\"-0\"+storms_df[\"CZ_TIMEZONE\"].str.extract(r'([0-9]+)')[0]+\"00\"`| This adds the time zone offset to the string by concatenating `-0` and the regex match for the time zone offset. Plus, it adds `00` to the end to represent the minutes.|\n",
    "|`format=\"mixed\"`|This tells pandas to try to parse the string in a mixed format, which means it will attempt to infer the format of the date and time.|\n",
    "|`utc=True`|This tells pandas to treat the datetime object as UTC time, which is a standard time zone that is not affected by daylight savings time.|\n",
    "|`.dt.tz_convert(\"-05:00\")`|This converts the datetime object to the specified time zone, which is Eastern Standard Time (EST) in this case.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the times look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df[\"BEGIN_DATE_TIME\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up exercises\n",
    "\n",
    "Here are some warm-up exercises to get started working with data that is a mix of categorical and datetime data. One helpful tip to working with datetime objects is to get the series of the datetime object and then use the `.dt` accessor to access the datetime properties. You can find all the properties of the datetime object [here](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of getting an attribute of a datetime object\n",
    "\n",
    "storms_df[\"END_DATE_TIME\"].dt.day_of_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using a method of a datetime object\n",
    "\n",
    "storms_df[\"END_DATE_TIME\"].dt.month_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three exercises are one-liners what will use the method `.value_counts()` to get the number of rows for each unique value in a column. This is a useful method to get a sense of the distribution of the data in a column. Below, we demonstrate by counting the number of each event type. (Compare this to the `groupby().size()` method we used last time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms_df[\"EVENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Are storms more common on weekends or weekdays? YES/NO. Display your answer by counting the number of events that happend on each day of the week. Don't worry about sorting the days of the week, just display the counts. Look at the datetime object documentation to see how to get the day of the week from the `.dt` accessor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "storms_df['BEGIN_DATE_TIME'].dt.day_name().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Which events tend to span more than one county? Display the counts of each event type that spans more than one county. (Hint: Events which span more than one county will have non-null values for `BEGIN_LOCATION` and `END_LOCATION`.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "storms_df[~storms_df[\"BEGIN_LOCATION\"].isna()][\"EVENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Which events tend to be multi-day events? Display the counts of each event type that has a duration of more than 1 day (24 hours).\n",
    ">\n",
    ">You will need to use the `pd.Timedelta()` function in your filtering criteria. Docs [here](https://pandas.pydata.org/docs/user_guide/timedeltas.html) and [here](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html#pandas.Timedelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "# Find duration of events\n",
    "storms_df['EVENT_DURATION'] = storms_df[\"END_DATE_TIME\"] - storms_df[\"BEGIN_DATE_TIME\"]\n",
    "\n",
    "# filter for events that last more than one day and count them\n",
    "storms_df[storms_df['EVENT_DURATION'] > pd.Timedelta(days=1)][\"EVENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Create two lists: a list of all county names and a list of all event types. How many counties and event types are there? You will be using the Series methods `.unique()` and `.tolist()` here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "county_list = storms_df[\"CZ_NAME\"].unique().tolist()\n",
    "print(county_list)\n",
    "print(f\"There are {len(county_list)} counties in the dataset\")\n",
    "event_list = storms_df[\"EVENT_TYPE\"].unique().tolist()\n",
    "print(event_list)\n",
    "print(f\"There are {len(event_list)} event types in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Function-writing warmup. Write a function called `get_event_county` that takes as input the storms dataframe object, a county name (as a string), and an event type (as a string). The function should return a dataframe that contains only that county and event type.\n",
    ">\n",
    "> This function is ideally a one-liner that filters the dataframe based on two criteria. Remember to enclose each condition in `()` and use the `&` operator to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your code\n",
    "# should return a dataframe with 7 rows\n",
    "get_event_county(storms_df, \"ELKHART\", \"Thunderstorm Wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "def get_event_county(storms, county, event):\n",
    "    return storms[(storms[\"EVENT_TYPE\"] == event) & (storms[\"CZ_NAME\"] == county)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported our data, cleaned it up and begun to explore it, let's split off into groups! We have a few paths we can take:\n",
    "\n",
    "1. A guided path: We will guide you through writing functions that summarize and print the data in different ways. This will help you practice writing functions, filtering data, and practice a tiny bit of plotting. \n",
    "2. A few self-guided exploratory questions that you can approach however you want, but focus on different skills:\n",
    "    * Question 1: What is the most dangerous county in Indiana? We provide a separate table of county info so you can normalize the data by county area. You will need to learn how to merge two dataframes together.\n",
    "    * Question 2: What is the best time to visit Indiana if you want to take cool pictures of clouds? This will help you practice summarizing data and, depending on how you approach it, work with date time objects of various duration. \n",
    "    * Question 3: Create a plot that compares the duration in hours of each common event type. This will help you practice creating new columns and plotting data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path 1: function writing practice\n",
    "\n",
    "1. Write a function called `storm_by_county` that takes as input the storms object, a **LIST** of county names, and returns a dataframe of all of the events that occurred in a given county and the date they began. Include an optional argument `storm_type` that by default is `None` and includes all event types, but if given a list, only includes those events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "# should return 8 rows of data\n",
    "storm_by_county(storms_df, [\"OHIO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "# should return 11 rows of data\n",
    "\n",
    "storm_by_county(storms_df, [\"ELKHART\", \"ST. JOSEPH\"], [\"Thunderstorm Wind\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "def storm_by_county(storms, county, storm_type=None):\n",
    "    if storm_type:\n",
    "        matches = storms[(storms['CZ_NAME'].isin(county)) & (storms['EVENT_TYPE'].isin(storm_type))]\n",
    "    else:\n",
    "        matches = storms[(storms['CZ_NAME'].isin(county))]\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function called `display_storms` that takes a storms object, iterates through each row, and prints out each event as a sentence, including the event type, the county, and the date and time of the event. This exercise helps with iterating over data. An example output would be: \"A thunderstorm occurred in Marion County on 2015-06-15 13:45:00\". (as a bonus, you can try and format the date and time to be more human-readable)\n",
    "\n",
    "**HINT:** You will need to use the `.iterrows()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test your function by:\n",
    "- Printing all storm events in TIPPECANOE county\n",
    "- Printing all \"Flood\" and \"Flash Flood\" events in the following counties: MARION, MONROE, SPENCER, VERMILLION\n",
    "- Print all \"Tornado\" events *in the whole state*, i.e. all the counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your storm_by_county function\n",
    "\n",
    "# 3.1: Display all storm events in TIPPECANOE county.\n",
    "print(\"--- 3.1 ---\")\n",
    "display_storms(storm_by_county(storms_df, [\"TIPPECANOE\"]))\n",
    "\n",
    "# 3.2: Display all Flood and Flash Flood events in the following counties: MARION, MONROE, SPENCER, VERMILLION\n",
    "print(\"--- 3.2 ---\")\n",
    "counties_to_check = [\"MARION\", \"MONROE\", \"SPENCER\", \"VERMILLION\"]\n",
    "storm_types = [\"Flood\", \"Flash Flood\"]\n",
    "\n",
    "display_storms(storm_by_county(storms_df, counties_to_check, storm_types))\n",
    "\n",
    "# 3.3: Display all Tornado events that occurred in the state.\n",
    "print(\"--- 3.3 ---\")\n",
    "county_list = storms_df[\"CZ_NAME\"].unique()\n",
    "display_storms(storm_by_county(storms_df, county_list, storm_type=[\"Tornado\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "def display_storms(storms):\n",
    "    for index, event in storms.iterrows():\n",
    "        print(\"A\", event[\"EVENT_TYPE\"], \"occurred on\", event[\"BEGIN_DATE_TIME\"], \"in\", event[\"CZ_NAME\"], \"county.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Make a histogram that displays the count of the events which contain \"flood\" in the following counties: [\"MARION\", \"MONROE\", \"SPENCER\", \"VERMILLION\", \"TIPPECANOE\"]. The x axis will be the different \"flood\" event types, and the color of the bar will be the county. The y axis will be the count of the events.\n",
    "\n",
    "HINT: First, you will have to find a way to get all values of `EVENT_TYPE` that contain the word \"flood\" (case insensitive) and make that into a list. Then, you can pass that list and your county list to your `storm_by_county()` function. There are many ways to get the values that contain \"flood\". One way is to use `str.contains()`, but will involve chaining a few methods and doing some dataframe subsetting. Another way is to use a for loop and an if statement and may be more readable. \n",
    "\n",
    "When you make a histogram using `sns.histplot()`, you can use the `multiple='dodge'` argument to make the bars side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "# import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# str.contains method\n",
    "storm_list = storms_df[\"EVENT_TYPE\"][storms_df[\"EVENT_TYPE\"].str.contains(\"flood\", case=False)].unique().tolist()\n",
    "\n",
    "# for loop method\n",
    "storm_list = []\n",
    "for event in storms_df[\"EVENT_TYPE\"].dropna().unique():\n",
    "    if \"flood\" in event.lower():\n",
    "        storm_list.append(event)\n",
    "\n",
    "# make a subset\n",
    "storms_subset = storm_by_county(storms_df, county=[\"MARION\", \"MONROE\", \"SPENCER\", \"VERMILLION\", \"TIPPECANOE\"], storm_type = storm_list)\n",
    "\n",
    "# plot the subset\n",
    "sns.histplot(storms_subset, x=\"EVENT_TYPE\", hue=\"CZ_NAME\", multiple=\"dodge\", shrink=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write another function called `summarise_storms` which takes as input the storms object you created, a list of counties, and an optional list of `storm_type` that displays only certain storm types (aka the same arguments as `storm_by_county`). Instead of returning every event, it returns a dataframe summarizing the number of occurences of each event in those counties. **HINT** You may want to use your previous function `storm_by_county` to avoid repeating code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test your function by:\n",
    "- Printing the number of all events in PIKE county without printing information about each one\n",
    "- Printing the total number of \"Thunderstorm Wind\" events in the following counties: ELKHART, LA PORTE, BOONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your modified storm_by_county function\n",
    "\n",
    "# 5.1 Display the number of all events in PIKE county\n",
    "print(\"--- 5.1 ---\")\n",
    "print(summarise_storms(storms_df, [\"PIKE\"]))\n",
    "\n",
    "# 5.2 Display the number of Thunderstorm Wind events in the following counties: ELKHART, LA PORTE, BOONE\n",
    "print(\"--- 5.2 ---\")\n",
    "print(summarise_storms(storms_df, [\"ELKHART\", \"LA PORTE\", \"BOONE\"], [\"Thunderstorm Wind\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "def summarise_storms(storms, county, storm_type=None):\n",
    "  df = storm_by_county(storms, county, storm_type).groupby([\"CZ_NAME\",\"EVENT_TYPE\"]).size().reset_index()\n",
    "  \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use your function to summarize the total counts of each weather event in each county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution {display-mode: \"form\"}\n",
    "\n",
    "all_weather = storms_df[\"EVENT_TYPE\"].unique().tolist()\n",
    "all_counties = storms_df[\"CZ_NAME\"].unique().tolist()\n",
    "\n",
    "summarise_storms(storms_df, all_counties, all_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-guided exploratory questions\n",
    "\n",
    "Here are a few questions we've come up with that can be approached in multiple different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** What is the most dangerous county in Indiana, assuming danger only comes from the storm events? You will need to account for the area of each county to answer this question. We've provided code to get the table from wikipedia that lists county information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml\n",
    "county_info = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_counties_in_Indiana\")\n",
    "county_info = county_info[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to do some data cleaning to get the county names to match up and to extract the area of each county. Then you will need to merge the two dataframes together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_info[[\"County\", \"Area[3][12]\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: data cleaning {display-mode: \"form\"}\n",
    "\n",
    "# cleaning county_info\n",
    "county_info[\"County\"] = county_info[\"County\"].str.replace(\" County\", \"\").str.upper()\n",
    "county_info[\"Area_sq_mi\"] = pd.to_numeric(county_info[\"Area[3][12]\"].str.extract(r'([0-9,]+)')[0])\n",
    "\n",
    "# merge the two dataframes\n",
    "storms_merged = pd.merge(storms_df, county_info, left_on=\"CZ_NAME\", right_on=\"County\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: example {display-mode: \"form\"}\n",
    "\n",
    "# grouping by county and area\n",
    "df = storms_merged.groupby([\"CZ_NAME\", \"Area_sq_mi\"])[\"EVENT_TYPE\"].count().reset_index()\n",
    "print(df.head())\n",
    "\n",
    "# normalizing number of events by area\n",
    "df[\"normalized\"] = df[\"EVENT_TYPE\"] / df[\"Area_sq_mi\"]\n",
    "\n",
    "# sorting by normalized value\n",
    "df.sort_values(\"normalized\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** What is the best time to visit Indiana if you want to take cool pictures of clouds? The main idea of this question is to summarize the data in such a way that you can tell which month or week or your choice of span of days has the highest concentration of events of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: example {display-mode: \"form\"}\n",
    "\n",
    "# decide which weather events probably have cool clouds\n",
    "cool_clouds = [\"Thunderstorm wind\", \"Tornado\", \"Lightning\", \"Heavy Rain\"]\n",
    "\n",
    "# filter for cool clouds\n",
    "df_clouds = storms_df[storms_df[\"EVENT_TYPE\"].isin(cool_clouds)]\n",
    "\n",
    "# group by month and number of events\n",
    "df_clouds.groupby(df_clouds[\"BEGIN_DATE_TIME\"].dt.month)[\"EVENT_TYPE\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Create a plot that compares the duration in hours of each common event type. Some events occur only infrequently, so your first step would be to filter those out. Then, you will need to decide what type of plot to make. Take a look at the [seaborn gallery](https://seaborn.pydata.org/examples/index.html). If you are having trouble deciding how to represent your data, take a look at this [infographic](https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/poster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: data parsing {display-mode: \"form\"}\n",
    "\n",
    "# get event duration. We've already done this, but the code is reproduced before\n",
    "\n",
    "storms_df[\"EVENT_DURATION\"] = storms_df[\"END_DATE_TIME\"] - storms_df[\"BEGIN_DATE_TIME\"]\n",
    "\n",
    "# filter out events which only occurred <10 times in the year\n",
    "\n",
    "storms_filtered = storms_df.groupby(\"EVENT_TYPE\").filter(lambda x: len(x) > 10)\n",
    "\n",
    "# group by event type and calculate average duration\n",
    "\n",
    "print(storms_filtered.groupby(\"EVENT_TYPE\")[\"EVENT_DURATION\"].mean())\n",
    "\n",
    "storms_filtered.groupby(\"EVENT_TYPE\")[\"EVENT_DURATION\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: plotting {display-mode: \"form\"}\n",
    "\n",
    "# plot the distribution of event durations as a stripplot with points using seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "storms_no_flood = storms_filtered[~storms_filtered[\"EVENT_TYPE\"].str.contains(\"Flood\")]\n",
    "\n",
    "# convert EVENT_DURATION to number of hours\n",
    "storms_no_flood[\"EVENT_DURATION_HOURS\"] = storms_no_flood[\"EVENT_DURATION\"].dt.total_seconds() / 3600\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = sns.stripplot(data=storms_no_flood, x=\"EVENT_DURATION_HOURS\", y=\"EVENT_TYPE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Solution: plotting pt 2 {display-mode: \"form\"}\n",
    "\n",
    "# plot the distribution of event durations as a stripplot with points using seaborn\n",
    "# separately plotting EVENT_TYPE = flood\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "storms_flood_only = storms_filtered[storms_filtered[\"EVENT_TYPE\"].str.contains(\"Flood\")]\n",
    "\n",
    "# convert EVENT_DURATION to number of hours\n",
    "storms_flood_only[\"EVENT_DURATION_HOURS\"] = storms_flood_only[\"EVENT_DURATION\"].dt.total_seconds() / 3600\n",
    "\n",
    "ax = sns.stripplot(data=storms_flood_only, x=\"EVENT_DURATION_HOURS\", y=\"EVENT_TYPE\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
