{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python data manipulation and visualization\n",
    "\n",
    "Let's begin by reviewing some of the terms we've covered in the past sessions that will come up again today.\n",
    "\n",
    "| Term | Definition |\n",
    "| --- | --- |\n",
    "| Object | The thing itself (an instance of a class) |\n",
    "| Variable | The name we give the object (a pointer to the object) |\n",
    "| Class | The blueprint for the object (defines the attributes and methods of object) |\n",
    "| Method | A function that belongs to an object |\n",
    "| Attribute | A property of an object |\n",
    "| Function | A piece of code that takes an input and gives an output/does something |\n",
    "| Argument| The objects that are passed to the function for it to operate on |\n",
    "| Library | Collections of python functions/capabilities that can be installed and loaded on top of base python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structures in general\n",
    "\n",
    "On our first day, we covered a few basic object types, such as strings, numbers, and booleans. Today when I talk about data, I mean the observations we have recorded as represented by one of these object types. This excludes for now things like images, audio, physical, or other data. Those can very much still be analyzed computationally (and by using python), but it is beyond the scope of this workshop. \n",
    "\n",
    "Today we'll mostly be focusing on various data structures, which are collections of individual observations. We've already seen lists, dictionaries, numpy arrays, and pandas dataframes, but why are there so many and how do they differ in the way we work with them? Why don't we just use Microsoft Excel for everything?\n",
    "\n",
    "Although you can look at your data in Excel, it's not a good platform for analyzing it. It is sluggish for large datasets, is not reproducible unlike code, it's difficult to automate, and the visualization capabilities are limited. You can also do more advanced analyses faster using python than using Excel. \n",
    "\n",
    "Python has various data structures because we interact with data in different ways and need different representations of them with their own set of capabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lists\n",
    "\n",
    "Lists are the most basic of data structures. They are created with `[]` and can contain any type of data. Each entry is separated by a comma. Lists are ordered and can be indexed, sliced, and concatenated just like strings. When lists are all numerical, they can also support mathematical operations like `max()` and `min()`. Lists can also be nested using another `[]` within the list. \n",
    "\n",
    "Lists are our first introduction to a **mutable** data structure, meaning you can change a list without having to create a new one. Indeed, list methods may modify your data **in place** and/or **return** a new object. If the method modifies the object in place, its return value will be `None`. Modifying in place means you don't have to assign the result of the method to a new variable, while returning a new object means you do have to assign it. For example, `list.append(x)` updates the list in place, while `list.pop()` both returns the last element and removes it from the list in place. \n",
    "\n",
    "Below are some useful operations and methods for lists. For a full list of methods, you can use `help()` on the list or consult the [docs](https://docs.python.org/3/library/stdtypes.html#list) page. \n",
    "\n",
    "**Operations and methods for lists**\n",
    "\n",
    "| Operation/Method | Description |\n",
    "| --- | --- |\n",
    "| `+` | Concatenation |\n",
    "| `*` | Repetition |\n",
    "| `[]`, `[:]` | Indexing, slicing |\n",
    "| `.append(x)` | Add `x` to the end of the list |\n",
    "| `.extend([x, y, z])` | Add `[x, y, z]` to the end of the list |\n",
    "| `.insert(i, x)` | Add `x` at index `i` of the list |\n",
    "| `.pop(i)` | Remove and return the element at index `i`, defaults to last element if none given |\n",
    "\n",
    "**Use cases for lists**\n",
    "\n",
    "Lists are a data structure that is always there in the background, being useful. We see them when creating simple ordered collections to iterate through, when we need to store a sequence of data to reference later, or when we need to collected a bunch of objects together. Think of lists as a small temporary transport for data. Lists are not good for large datasets (because it will be slow) or when you need to do a lot of mathematical operations (because it lacks functionality). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "Dictionaries store key:value pairs. Keys are typically strings or numerical identifiers, while the values can be just about anything, including other dictionaries, lists, or individual values. You can create a dictionary with `{}` or with the `dict()` function. The two ways to create a dictionary are shown below:\n",
    "\n",
    "```python\n",
    "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "my_dict = dict((\"a\", 1), (\"b\", 2), (\"c\", 3))\n",
    "```\n",
    "\n",
    "Dictionaries are unordered, so you can't index/slice them. But you can retrieve items by their key, e.g. `my_dict[\"a\"]` or `my_dict.get(\"a\")`. Like lists, dictionaries are mutable, so you can add, remove, or update the key:value pairs in place. Other methods return \"View objects\" that allow you to see the items in the dictionary, but won't allow you to modify the dictionary. Here are some useful methods for dictionaries:\n",
    "\n",
    "**Operations and methods for dictionaries**\n",
    "\n",
    "| Operation/Method | Description |\n",
    "| --- | --- |\n",
    "| `[]` | Retrieve value by key |\n",
    "| `.keys()` | Returns a view object of the keys |\n",
    "| `.values()` | Returns a view object of the values |\n",
    "| `.items()` | Returns a view object of the key:value pairs |\n",
    "| `.update(dict)` | Updates the dictionary with the key:value pairs from another dictionary |\n",
    "\n",
    "**Use cases for dictionaries**\n",
    "\n",
    "Dictionaries are a data structure that is more specialized for information that can be organized in a key:value pair way. You may see dictionaries being used to store associations between a name/ID and some characteristics, or to store a set of parameters for a function, or to organize a hierarchical grouping of information. Dictionaries are optimized for fast access to the values by key and for flexible organization of the data. Although you can edit the values of a dictionary, they aren't good for mathematical operations or for ordered data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "### Numpy arrays\n",
    "\n",
    "Numpy arrays are a data structure that only contain one type of data, typically numerical, and are N-dimensional (any number of dimensions). You can create numpy arrays using the `np.array()` function or by converting other data structures to an array using `np.asarray()` or other helper functions. There are also many functions that can create an array with pre-filled numbers, such as `np.zeros()` and `np.arange()`. An array is defined by its `shape`, which describes the number of elements in each dimension, also known as axes. The first axis is the number of rows, the second is the number of columns, and so on. \n",
    "\n",
    "Today we will be spending a lot of time with numpy arrays as they are one of the main data structures for working with numerical data. We will learn how to navigate them, read and write from them, and also how to perform mathematical operations on them. We will also use numpy to create some visualizations. \n",
    "\n",
    "**Use cases for numpy arrays**\n",
    "\n",
    "The use cases for numpy arrays are very broad. They are used in scientific computing, machine learning, data analysis, and more. They are optimized for fast mathematical operations and are very efficient in terms of memory usage. They are also very flexible in terms of the number of dimensions they can have, so you can store a lot of data in a single numpy array. They are not good for storing mixed data types or for storing data that is not numerical. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting\n",
    "\n",
    "You can perform mathematical operations on arrays and they'll propagate to each element. This is called **broadcasting**. In order for the element-wise operation to work, the two objects you're operating with either have to have the same shape or one of them has to be a scalar. Numpy also has functions that allow you to operate on the entire array, such as `np.sum()`, `np.mean()`, etc. In the last session, we wrote a function to calculate the mean squared error of two 1D arrays using the formula $\\frac{1}{n}\\sum_{i=1}^{n}(predicted_i - expected_i)^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array([1, 2, 3, 4, 5])\n",
    "expected = np.array([2, 4, 1, 5, 5])\n",
    "mse = (1/len(predicted)) * np.sum(np.square(predicted - expected))\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down the code, you can see what is produced at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted - expected)\n",
    "print(np.square(predicted - expected))\n",
    "print(np.sum(np.square(predicted - expected)))\n",
    "print(1/len(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing and indexing numpy arrays\n",
    "\n",
    "Slicing arrays is a powerful tool for extracting subsets of data. The syntax for slicing is `array[start:stop:step]`. If you don't specify a start, it defaults to 0, if you don't specify a stop, it defaults to the end of the array, and if you don't specify a step, it defaults to 1. When you specify a start and stop, the range is inclusive of the start and exclusive of the stop. Usually, you don't need to specify the step, so you can omit the second colon. \n",
    "\n",
    "You can also use negative indices to count from the end of the array. When you use negative indices, inclusive of the start and exclusive of the stop still applies.\n",
    "\n",
    "Multi-dimensional numpy arrays are sliced and indexed with the same syntax, but you need to separate the dimensions with a comma. For example, `array[i, j]` will return the element at row `i` and column `j`. In two dimensions, the first axis is the rows and the second axis is the columns. So now the syntax is `array[row_start:row_stop:row_step, col_start:col_stop:col_step]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of slicing one-dimensional and two-dimensional arrays. See if you can predict the output before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of slicing one-dimensional arrays\n",
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "print(arr1[0:2])\n",
    "print(arr1[1:3])\n",
    "print(arr1[:])\n",
    "print(arr1[2:])\n",
    "print(arr1[-1:])\n",
    "print(arr1[:-1])\n",
    "print(arr1[-3:-1])\n",
    "print(arr1[::2])\n",
    "print(arr1[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice slicing the following array of 25 elements reshaped into a 5x5 array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10],\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20],\n",
    "    [21, 22, 23, 24, 25]\n",
    "])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Perform the following operations on the `arr` array:\n",
    ">\n",
    "> 1. Extract the first row\n",
    "> 2. Extract the last column\n",
    "> 3. Extract the first three rows\n",
    "> 4. Extract the central 3x3 square\n",
    "> 5. Extract the last column using positive indexing, and then do it again using negative indexing\n",
    "> 6. Extract every other column (and all rows)\n",
    "> 7. Extract every other element of the first row. You should get [1, 3, 5]\n",
    "> 8. Extract the last row and reverse it. You should get [25, 24, 23, 22, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the first row of the array\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract the last column\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract the first three rows\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract the central 3x3 square\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Extract the last column using positive indexing, then do it again using negative indexing\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Extract every other column (and all rows)\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Extract every other element of the first row. You should get [1, 3, 5]\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Extract the last row and reverse it. You should get [25, 24, 23, 22, 21]\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Bonus Exercises**: Here are some more challenging slicing exercises:\n",
    ">\n",
    "> B1. Extract every other column and every other row, starting with the number 1. You should get:\n",
    "> ```\n",
    "> [[ 1  3  5]\n",
    ">  [11 13 15]\n",
    ">  [21 23 25]]\n",
    "> ```\n",
    "> B2. Extract a checkerboard pattern starting from the number 2. You should get:\n",
    "> ```\n",
    "> [[ 2  4]\n",
    ">  [12 14]\n",
    ">  [22 24]]\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1. Extract every other column and every other row, starting with the number 1.\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2. Extract a checkerboard pattern starting from the number 2.\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that slicing arrays creates a **view** of the original array, not a copy. This is known as \"passing by reference\". That means if you use a slice of an array and modify it, the original array will also be modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original array\n",
    "print(arr)\n",
    "# This is my slice of the array\n",
    "slice = arr[1:4,1:4]\n",
    "print(slice)\n",
    "# I'm going to change the slice\n",
    "slice[:,:] = 999\n",
    "print(slice)\n",
    "# This is the original array\n",
    "print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the array\n",
    "arr = np.array([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10],\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20],\n",
    "    [21, 22, 23, 24, 25]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do a calculation on a slice of an array, the result will be a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the original array\n",
    "print(arr)\n",
    "# This is my slice of the array\n",
    "slice = arr[1:4,1:4]\n",
    "print(slice)\n",
    "# add 100 to each element of the slice\n",
    "slice = slice + 100\n",
    "print(slice)\n",
    "# This is the original array\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using boolean masks to filter arrays\n",
    "\n",
    "You can use boolean expresssions to filter arrays. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the even numbers from the array\n",
    "arr[arr % 2 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, numpy creates a boolean mask that is the same shape as the array. The mask is `True` where the condition is met and `False` where it is not. You can then use the mask to filter the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr % 2 == 0)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using boolean masks in this way, it's important to note that the original shape of the array is not preserved. So this is useful if you want to filter out elements and do something with that collection of elements, but not if you want to replace certain elements with others. \n",
    "\n",
    "If you want to replace elements in an array based on a condition, you can use the `np.where()` function. This function takes three arguments: the condition, the value to use if the condition is `True`, and the value to use if the condition is `False`.\n",
    "\n",
    "In the code block below, we will replace all of the odd numbers in the array with the number 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(arr % 2 == 0, arr, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Perform the following operations on the `arr` array using boolean masks:\n",
    ">\n",
    "> 1. Extract all values less than the mean\n",
    "> 2. Trim the array by removing the maximum and mininum value\n",
    "> 3. Replace the maximum and minimum values with the mean of the entire array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all values less than the mean\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim the array by removing the maximum and mininum value. Use np.min() and np.max() to find the min and max values\n",
    "# Hint: it might be easiest to explicitly create the boolean mask for this\n",
    "# The operator & is the element-wise AND operator, it returns True if both elements in an array are True\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the maximum and minimum values with the mean of the entire array\n",
    "# Hint: you can use the mask you created above and np.where()\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy math functions and additional capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy library isn't just about arrays. It also has a lot of mathematical functions that can be applied to arrays. Some of these functions may seem like duplicates of base python functions, but they are optimized for operating on multi-dimensional arrays whereas the base python functions are not. Other functions are unique to numpy and offer more advanced mathematical capabilities.\n",
    "\n",
    "You can find a list of all mathematical functions in the numpy library [here](https://numpy.org/doc/stable/reference/routines.math.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean of the array along the columns\n",
    "np.mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the mean along the rows\n",
    "np.mean(arr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the standard deviation along the columns\n",
    "np.std(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting histogram of the array\n",
    "np.histogram(arr, bins=5, range=(1, 25))\n",
    "# if you don't want to hardcode the range, you can use the min and max functions\n",
    "np.histogram(arr, bins=5, range=(np.min(arr), np.max(arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take a short break from numpy to learn how to read and write data from files. Then, we will bring it back to numpy to learn how to read and write numpy arrays. Reading and writing files is a very important step in data analysis. For example, some raw files you have may be structured in a way that isn't machine-readable. Parsing or fixing those files is often easier using python or another coding language than re-doing it by hand. You may find yourself generating data or calculations that you want to save to your local machine. Using code to read and write files enhances reproducibility and automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data by line\n",
    "\n",
    "A common way to read data is by line. This is useful when you have a file that is too large to fit into memory all at once. You can read the file line by line and process each line as you go. This is also useful when you need to parse a file that has a specific structure so that it can be read into a data structure like a numpy array or pandas dataframe. In this section, when we talk about files, we mean text files that contain data that exist on your local machine. This is different from the data structures we've been working with so far, which are in memory (in your python instance).\n",
    "\n",
    "The syntax for opening a file is `open(filename, mode)`, where mode can be `r` for reading, `w` for writing, `a` for appending. Reading mode means that you can only read the file but not change it (on the disk). Writing mode means you are creating a new file or overwriting an existing file. Appending mode means you are adding to an existing file.\n",
    "\n",
    "When you open a file, you can read it line by line using a for loop. Normally, when you open a file, you should also close it with the `close()` method. However, a more \"pythonic\" way to do this is to use a `with` statement, which will automatically close the file once you've done what you need. Here's an example of how that might look with a for loop to print each line of a file:\n",
    "\n",
    "```python\n",
    "with open('filename.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "```\n",
    "\n",
    "The above code will print each line of the file to the console. The variable `file` is how we refer to the file object that we've opened. We can name it anything we want, but `file` was just an example. The `line` variable is how we refer to each line of the file as we iterate through it. Again, it's an arbitrary name. This for loop is similar to how we iterate through a list. The only difference is that we're iterating through something that is being read from our local computer rather than an object in memory.\n",
    "\n",
    "In the example, we are only printing the lines of the file, but just as with any for loop, you can do anything you want with each line, such as apply a function to it, split it up and store it in a data structure, or write different parts of it to a new file. So think of that `print(line)` line as a placeholder for whatever you want to do with the line.\n",
    "\n",
    "#### Vocab\n",
    "\n",
    "|Term|Definition|\n",
    "|---|---|\n",
    "|File|A collection of data stored on a disk|\n",
    "|Line|A string of characters that ends with a newline character|\n",
    "|Newline character|A special character that indicates the end of a line, usually `\\n`|\n",
    "|Delimiter|A character that separates data fields in a line, usually a comma, tab, or space|\n",
    "|Parsing|The process of extracting data from a file|\n",
    "|Whitespace|Any character that represents a space, tab, or newline|\n",
    "|Leading/trailing whitespace|Whitespace at the beginning or end of a string|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a more concrete example of how we might read and then parse through a file. Let's suppose we have a list of taxon ids and bird names that we want to read into a dictionary. The file looks like this:\n",
    "\n",
    "```\n",
    "Anas rubripes,American Black Duck,6924\n",
    "Fulica americana,American Coot,473\n",
    "Spinus tristis,American Goldfinch,145310\n",
    "Falco sparverius,American Kestrel,4665\n",
    "```\n",
    "\n",
    "In the code below we first read the file line by line, then strip the whitespace and split the line by a comma. Then, we will create a dictionary where the key is the taxon id and the value is the common name of the bird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/bird_names.csv'\n",
    "\n",
    "bird_names = dict()\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip().split(',')\n",
    "        bird_names[line[2]] = line[1]\n",
    "\n",
    "print(bird_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Rerun the code above and remove the `strip()` method. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Copy the code above and modify it so that the dictionary keys are the taxon ids and the values are another dictionary, with keys 'scientific_name' and 'common_name' and values the appropriate entries for that bird species.\n",
    ">\n",
    "> For example, a sample dictionary entry should look like this:\n",
    "> ```\n",
    "> {6924: {'scientific_name': 'Anas rubripes', 'common_name': 'American Black Duck'}}\n",
    "> ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Why did we use a dictionary to store the data in the previous exercise? Think about what features of a dictionary make it a good choice or what features of lists or arrays make them a bad choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an excerpt from a file of iNaturalist observations of birds in Cambridge, MA from the year 2023. We will loop through the file and count the number of observations of each species. We will also use the previously created dictionary to get the species names.\n",
    "\n",
    "```csv\n",
    "id,time_observed_at,taxon_id\n",
    "145591043,2023-01-01 17:33:31 UTC,14886\n",
    "145610149,2023-01-01 20:55:00 UTC,7004\n",
    "145610383,2023-01-01 21:13:00 UTC,6993\n",
    "145611915,2023-01-01 21:12:00 UTC,13858\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/bird_observations.csv'\n",
    "\n",
    "bird_observations = dict()\n",
    "\n",
    "with open(filename, 'r') as birdfile:\n",
    "    # skip the header\n",
    "    next(birdfile)\n",
    "    for line in birdfile:\n",
    "        # clean up the line and split into list\n",
    "        observation = line.strip().split(',')\n",
    "        # get the bird id\n",
    "        id = observation[2]\n",
    "        # get the bird name by looking up in the bird_names dictionary\n",
    "        name = bird_names[id]['common_name']\n",
    "        # if this is the first time we're seeing the bird, add it to our observations dict\n",
    "        if name not in bird_observations:\n",
    "            bird_observations[name] = 0\n",
    "        # increment the count by 1\n",
    "        bird_observations[name] += 1\n",
    "print(bird_observations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you routinely find yourself reading delimited files, you might want to use the `csv` library. The `csv` library also has the ability to parse Excel files or read and write to/from dictionaries directly. For more information, here's the [doc page](https://docs.python.org/3/library/csv.html). Here's what the above code would look like using the `csv` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/bird_observations.csv'\n",
    "\n",
    "bird_observations = dict()\n",
    "\n",
    "with open(filename, 'r') as birdfile:\n",
    "    # this line takes the place of us having to strip and split the lines\n",
    "    reader = csv.reader(birdfile, delimiter=',')\n",
    "    # skip the header\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        id = row[2]\n",
    "        name = bird_names[id]['common_name']\n",
    "        if name not in bird_observations:\n",
    "            bird_observations[name] = 0\n",
    "        bird_observations[name] += 1\n",
    "print(bird_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some handy functions when working with lines in files. These are all string methods, so you can use them on any string, including strings that are read from a file.\n",
    "\n",
    "**Useful functions for reading files by line**\n",
    "\n",
    "| Function | Description |\n",
    "| --- | --- |\n",
    "| `.strip()` | Removes leading and trailing whitespace and newlines from a string |\n",
    "| `.split()` | Splits a string into a list of strings based on a delimiter |\n",
    "| `.join()` | Joins a list of strings into a single string with a delimiter |\n",
    "| `line[:]` | Indexing and slicing works on strings too |\n",
    "| `.replace(old, new)` | Replaces all instances of `old` with `new` in a string |\n",
    "\n",
    "**Useful special characters**\n",
    "\n",
    "Special characters in files are often used as delimiters or to indicate the end of a line. The two most common special characters are:\n",
    "\n",
    "| Character | Description |\n",
    "| --- | --- |\n",
    "| `\\n` | Newline character |\n",
    "| `\\t` | Tab character |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data by line\n",
    "\n",
    "Writing data to a file is similar to reading data from a file. You can open a file in write mode and then write to it line by line using the `print()` method, but this time passing in the variable we've stored the opened file in (in our case the variable is unimaginatively named `file`). Here's an example of writing a list of strings to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = ['this is a test', 'this is another test', 'this is the final test']\n",
    "\n",
    "with open('my_text.txt', 'w') as file:\n",
    "    for line in my_text:\n",
    "        print(line, file=file)\n",
    "\n",
    "# reading it back\n",
    "with open('my_text.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: When we print the file back, there are extra newlines in between each line. Can you think of a way to display the contents of the file without these? There are several options. If you are stuck, look at the documentation of the [print()](https://docs.python.org/3/library/functions.html#print) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing data with numpy\n",
    "\n",
    "Unless we're simulating data, most of the time we'll be loading data into python from a file. Numpy has two main functions for loading in text-based data such as CSVs: `np.loadtxt()` and `np.genfromtxt()`. The main difference between the two is that `np.genfromtxt()` can handle missing data, while `np.loadtxt()` cannot. So, to play it safe, it's usually best to use `np.genfromtxt()`.\n",
    "\n",
    "Remember that numpy arrays can only contain one type of data, so if your file contains headers or mixed data types, you'll need to use additional options to tell numpy how to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now import a numerical dataset of red wine quality ratings and various chemical properties. It has a header row and is separated by semi-colons. Here's a preview of the data:\n",
    "\n",
    "```\n",
    "fixed acidity;volatile acidity;citric acid;residual sugar;chlorides;free sulfur dioxide;total sulfur dioxide;density;pH;sulphates;alcohol;quality\n",
    "7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5\n",
    "7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import using genfromtxt and skip the header\n",
    "wines_array = np.genfromtxt('https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/winequality-red.csv', delimiter=';', skip_header = 1)\n",
    "# import the header separately\n",
    "# we specify the dtype as str so that numpy doesn't try to convert the header to a number\n",
    "wines_header = np.genfromtxt('https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/winequality-red.csv', delimiter=';', max_rows=1, dtype=str)\n",
    "print(wines_header)\n",
    "print(wines_array)\n",
    "print(wines_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Use `max` or `np.max` to find the max quality rating in the dataset and extract the rows with that rating. Save it to a new array called `best_wines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_wines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have 18 top wines, let's save them to a new file. We can use the `np.savetxt()` function to save it as a human-readable delimited file. Instead of using semi-colons, we'll use commas to separate the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the array to a csv\n",
    "np.savetxt(\"best_wines.csv\", best_wines, delimiter=\",\")\n",
    "# Reading it back\n",
    "with open('best_wines.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to the numbers? The default format for `np.savetxt()` is to save the data as a floating point number with 8 decimal places. However, our original csv did not have that much precision. This is because numpy internally loaded our data in this format but just didn't display the whole thing for us. We can change a number's **display format** using the `fmt` argument. Here's how you would save the data with 3 decimal places. You will also notice that we took the `wines_header` variable and joined it into a single string with commas. This is because `np.savetxt()` expects a single string for the header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"best_wines.csv\", best_wines, delimiter=\",\", fmt='%.3f', header=','.join(wines_header))\n",
    "with open('best_wines.csv', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of saving the data prepends the header row with a `#` character to indicate that it is a comment. That way, if you were to load the data back in with numpy, it would ignore the header row automatically. If you don't want this behavior, you can set the `comments` argument to an empty string. There are a lot of formatting options if you need things to be precisely formatted in print, but we won't go into the details today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Now that we've got the data, let's do one exercise focused on manipulating the data within numpy arrays. Remember what we've learned about slicing arrays and broadcasting operations. \n",
    ">\n",
    "> Every chemical property on the wine dataset is measured in different units. Let's normalize all the data so that it is on the same scale. To do this, we will first subtract the mean (`np.mean`) of each column from each element in the column, then divide by the standard deviation of the column (`np.std`). The quality column (last column) is not a chemical property, so we will not normalize it. Save the normalized data + original quality column to a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# make a reference to the wines array without the last column\n",
    "\n",
    "# calculate the mean of each column\n",
    "\n",
    "# calculate the standard deviation of each column\n",
    "\n",
    "# calculate the z-scores and assign them to a new array, z_scores\n",
    "\n",
    "# add back the quality column to the z_scores array (hint: use np.column_stack)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Bonus exercise**: Can you figure out a way to do this, but without assuming the last column is the quality? One approach would be to first identify the column index of the 'quality' column in the header array (you can use np.where for this, or convert to a list and use index). Then, make a column mask that includes all columns but the one that matches the quality index. Then, make your slice using the column maks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "\n",
    "Matplotlib is probably the most popular library for plotting figures in python. It is the basis of other plotting libraries, such as seaborn and the pandas dataframe plots. In this section we will demonstrate how to plot with matplotlib to illustrate the underlying principles of plotting.\n",
    "\n",
    "There are two ways to plot using matplotlib: the object-oriented interface and the pyplot interface. The pyplot interface was designed to help folks transitioning from MATLAB to python. The object-oriented interface is the more reccommended way to plot figures if you're completely new. To begin plotting, we first have to import pyplot from matplotlib, which is typically done like `import matplotlib.pyplot as plt`. This allows us to use the `plt` alias to access the pyplot functions.\n",
    "\n",
    "Simple plots such as scatter and line plots with one or two variables are easy to create in matplotlib. The steps to plotting are as follows:\n",
    "\n",
    "1. Create a figure and Axes object using `plt.subplots()`\n",
    "2. Use the Axes object to plot the data using one of the plot methods such as `.scatter()`, `.bar()`, etc. \n",
    "3. Customize the plot using the Axes object's `.set_...` methods\n",
    "4. Add a legend using `ax.legend()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "# generate some uniformly distributed x values\n",
    "expected = np.random.randint(1, 10, 20)\n",
    "# add some normally distributed noise to get y values\n",
    "predicted1 = expected + np.random.normal(0, 1, 20)\n",
    "predicted2 = expected + np.random.normal(2, 1, 20)\n",
    "\n",
    "# Create a subplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot each group. You can use `label` to add a legend to each group of points\n",
    "ax.scatter(x = expected, y = predicted1, label = \"Group 1\")\n",
    "ax.scatter(x = expected, y = predicted2, label = \"Group 2\")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xlabel(\"Expected\")\n",
    "ax.set_ylabel(\"Predicted\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: Create a scatter plot of the first two columns of the `wines_array` dataset. Label the x-axis \"Fixed Acidity\" and the y-axis \"Volatile Acidity\". Add a title to the plot \"Fixed vs Volatile Acidity in Red Wines\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Create the fix, ax object\n",
    "\n",
    "# Plot the data with ax.scatter\n",
    "\n",
    "# Label the x-axis and y-axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ax.scatter()` method takes in the x and y data as the first two arguments. There are additional arguments you can pass to customize the plot, such as changing the color (`c`) or size (`s`) of the points based on a third variable. If you want the legend to reflect the color of points, you can save the return object of the `ax.scatter()` method to a variable and pass that to the `ax.legend()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the data with ax.scatter\n",
    "pts = ax.scatter(x = wines_array[:,0], y = wines_array[:,1], c = wines_array[:,-1])\n",
    "\n",
    "# the pts object has a method for getting information about the legend\n",
    "print(pts.legend_elements()[0]) # this element represents the points (each unique dot type)\n",
    "print(pts.legend_elements()[1]) # this element represents the values (quality)\n",
    "\n",
    "# Use the * operator to unpack the tuple into the legend method.\n",
    "ax.legend(*pts.legend_elements(), title=\"Quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using loops to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is relatively simple to plot two axes in one figure. Give `plt.subplots` two numbers representing the number of rows and columns you want and the Axes object it returns will beceome an array. You can then index into it to plot on each object. In the below example, we also demonstrate how to plot text using the `.text()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].text(0.5, 0.5, \"Top plot\", ha = \"center\")\n",
    "ax[1].text(0.5, 0.5, \"Bottom plot\", ha = \"center\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to plot multiple subplots, pass the number of rows and columns of subplots you want to the `plt.subplots()` function. This will return an array of axes objects which you can index into to plot on each subplot. For larger plots, it's helpful to increase the size of the canvas using the `figsize` parameter in `plt.subplots()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of looping through axes to plot multiple subplots\n",
    "fig, ax = plt.subplots(3, 3, figsize = (10, 10))\n",
    "# This flattens the 2D array of axes into a 1D array\n",
    "# Alternatively, you can use a nested loop to plot by row and column\n",
    "ax = ax.flatten()\n",
    "for i in range(9):\n",
    "    # draw number in center of plot\n",
    "    ax[i].text(0.5, 0.5, str(i), fontsize = 18, ha = 'center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use loops to loop through an array of data and plot each column. This is useful when you have a lot of data and don't want to write out each plot individually. Let's start with a new type of plot, histogram. The `ax.hist()` method creates a histogram, which is a plot of the frequency of data points in a certain range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(wines_array[:,0], bins = 20, label = wines_header[0])\n",
    "ax.set_title(\"Fixed Acidity\")\n",
    "ax.set_xlabel(\"Z Score\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will loop through the columns of the `wines_array` dataset and plot a histogram of each column. We will also add a title to each subplot with the name of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "\n",
    "num_columns = wines_array.shape[1]\n",
    "fig, axes = plt.subplots(num_columns, 1, figsize=(10, num_columns * 3), constrained_layout=True)\n",
    "\n",
    "for i in range(num_columns):\n",
    "    ax = axes[i]  # Select the current subplot\n",
    "    ax.hist(wines_array[:, i], bins=30, alpha=0.7, edgecolor='black')  # Plot histogram of the current column\n",
    "    ax.set_title(f'Histogram of {wines_header[i]}')  # Set title\n",
    "    ax.set_xlabel('Z Score')  # Set x-axis label\n",
    "    ax.set_ylabel('Frequency')  # Set y-axis label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** You will notice that the last column we plotted was the quality scores, and we mistakenly mis-labeled the X axis as \"Z Score\". Copy the code above and fix the final x-axis label to \"Quality Score\". *hint* you can fix it outside of the loop or inside the loop using an if statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just scratching the surface of what you can do with matplotlib. The way most people learn to use matplotlib for their own purposes is to look at the [gallery](https://matplotlib.org/stable/gallery/index.html) and find a plot that is similar to what they want to make. Then they copy the code and modify it to suit their needs.\n",
    "\n",
    "Alternatively, you can usually find the answer to your questions online. While matplotlib is very powerful, it can be tricky to use and most people have to look up how to do things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Bonuse Exercise**: Think about how you might use a for loop to plot multiple sets of points in the same scatter plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "### The pandas DataFrame\n",
    "\n",
    "Often when we work with data, we have multiple variables with different data types. While numpy arrays are very flexible for numerical data, it does not work as well with a combination of numerical and categorical data or when you want to have labels for your rows and columns. Pandas is another python library that builds upon numpy and adds the ability to create DataFrames, which are 2D tables with labeled rows and columns. You can think of python DataFrames as spreadsheets from Excel or dataframes from R. \n",
    "\n",
    "Let's manually create a simple dataframe in pandas to showcase their behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tournamentStats = {\n",
    "    \"wrestler\": [\"Terunofuji\", \"Ura\", \"Shodai\", \"Takanosho\"],\n",
    "    \"wins\": [13, 6, 10, 12]\n",
    "}\n",
    "\n",
    "dataframe = pd.DataFrame(tournamentStats)\n",
    "\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that in pandas dataframes, both rows and columns are *explicitly indexed*, which means that every row and column has a label associated with it, i.e. the index. You can think of the explicit indices as the being the names of the rows and the names of the columns. In other libraries e.g. `numpy` or indeed in base python, arrays and similar data structures are *implicitly* indexed, which means they have an integer associated with their position. For example, if we wanted the first element in a list we would refer to it like `lst[0]`, or we wanted the first element in the first row of a 2D `numpy` array we could specify it with `array[1,1]`. With pandas dataframes, you CAN refer to values implicitly using this default integer index, but you can also refer to its explicit index label. \n",
    "For example, we can check the index of the columns in a pandas dataframe using the `columns()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can refer to a specific column with its index inside of square brackets `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single column:\n",
    "dataframe[\"wrestler\"]\n",
    "\n",
    "#Multiple columns (note the double []!):\n",
    "dataframe[[\"wrestler\", \"wins\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to explicitly refer to a row, we need to use the `.loc` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's check the row indices using the index() function:\n",
    "print(dataframe.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And we can refer to a row using its specific label:\n",
    "dataframe.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to use `.loc` if we are referring to a specific row AND column, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframe.loc[0, \"wrestler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing real data with pandas\n",
    "\n",
    "To best illustrate the strength of pandas, let's consider a practical use-case. Let's say that we have conducted an RNAseq experiment. We sequenced RNA from five samples, and we want to know how expression levels of each transcript differ between our five samples. We map the RNAseq data against our reference transcriptome and use the `Kallisto` software package to actually calculate the expression level of each transcript in our transcriptome. For each sample, `Kallisto` will output a tab-separated list (a `.tsv` file) that looks something like this: \n",
    "\n",
    "```\n",
    "target_id\tlength\teff_length\test_counts\ttpm\n",
    "TCONS_00000001\t1809\t1611.2\t0\t0\n",
    "TCONS_00000002\t1134\t936.204\t0\t0\n",
    "TCONS_00000003\t1118\t920.204\t0\t0\n",
    "TCONS_00000004\t1121\t923.204\t0\t0\n",
    "TCONS_00000131\t792\t594.718\t0\t0\n",
    "TCONS_00000005\t1429\t1231.2\t0\t0\n",
    "TCONS_00000006\t1554\t1356.2\t0\t0\n",
    "TCONS_00000007\t1662\t1464.2\t6.00886\t0.166434\n",
    "TCONS_00000132\t1997\t1799.2\t0\t0\n",
    "```\n",
    "\n",
    "Let's not worry about the details of each column, as for our purposes the two columns were are interested in are the first and last, i.e. the 'target_id' column and the 'tpm' columns. 'Target_id' refers to the unique ID for each transcript in our transcriptome (which are called \"TCONS_someNumber\"); 'tpm' stands for \"transcripts per million\" and is just a normalized measurement of transcript abundance (i.e. high TPM == a highly expressed trancript).\n",
    "\n",
    "Since we have five samples, we will have 5 `.tsv` files output by `Kallisto`. *We want to create a final summary dataset that has a column of transcript IDs and columns for the TPM for each sample, AND the gene that each transcript derives from.* \n",
    "\n",
    "This is the sort of problem you will run into quite a lot in bioinformatics...a decently sized portion of the field can basically be summed up as \"take this file and re-arrange it to look like another file\"! I often find it helpful when starting on tasks like this to *envision what I want my final data to look like*, which can help me figure out where to begin. So in this case, I know I want to produce a final table that looks something like this:\n",
    "\n",
    "| transID   | sample1_TPM | sample2_TPM | sample3_TPM | sample4_TPM | sample5_TPM | geneID |\n",
    "|-----------|-------------|-------------|-------------|-------------|-------------|--------|\n",
    "| TCONS_01  | 0           | 0           | 1.2         | 5           | 3           | gene1  |\n",
    "| TCONS_02  | 21.4        | 12.3        | 10.0        | 0           | 9.3         | gene1  |\n",
    "| TCONS_03  | 3.3         | 0.12        | 0           | 0           | 4.7         | gene2  |\n",
    "| Etc. etc. |             |             |             |             |             |        |\n",
    "\n",
    "You might be confused about where the gene IDs are coming from, as if we look at our input `.tsv` files we can see that they do not contain any **gene level information**, only the transcript IDs. It's true, we will need an additional source of information! We also have the file `data/geneIDs.tsv`, which contains the corresponding gene names for each transcript and looks like this:\n",
    "\n",
    "```\n",
    "TCONS_00000001  gene-SNX18_gGal-2\n",
    "TCONS_00000002  gene-LOC116806879_tGut-like-2\n",
    "TCONS_00000004  gene-KCMF1_gGal-like-1\n",
    "TCONS_00000003  gene-KCMF1_gGal-like-1\n",
    "TCONS_00000005  gene-ATP5F1AW_gGal-like-2\n",
    "Etc. etc.\n",
    "```\n",
    "So once we have our re-arranged data frame, we will need to query this list and pull out the gene name for each transcript. This can all feel a little overwhelming, so let's step back and consider the large-scale things that our code will need to accomplish...in other words, let's write the logic flow out as pseudo-code together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Pseudo-code for our task:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data to pandas\n",
    "One of the most useful features of pandas DataFrames is its ability to easily perform complex data transformations. This makes it a powerful tool for cleaning, filtering, and summarizing tabular data. Let's read some data into a DataFrame to demonstrate. \n",
    "\n",
    "Below you can see an example of how to read files into pandas using the `pd.read_csv()` function. The `csv` stands for 'comma-separated values', which means by defaults it will assume that our columns are separated by **commas**; however, we can change that to tab-separated easily, using the `sep=` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/A01v1_20908_bl_S1_L003_abundance.tsv\", sep='\\t')\n",
    "\n",
    "# The head() function from pandas prints only the first N lines of a dataframe (default: 10)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to do the same task several times (i.e. turn multiple input files into pandas dataframes), let's write a function that takes a sample ID and a file path and does the following:\n",
    "1. use the `read_csv()` function in pandas to read in a file as a pandas dataframe (remember: our input is TAB separated, not comma separated!)\n",
    "2. re-name the 'target_id' column to 'transcriptID' and the 'tpm' column to the sample ID\n",
    "3. subsets the data frame to only keep the two renamed columns\n",
    "4. return the simplified data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you can give a list of columns to a dataframe to subset that dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_cols = ['target_id', 'tpm']\n",
    "df1[list_of_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise**: write a function that takes a sample ID and a file path and does the following:\n",
    ">\n",
    ">1. use the `read_csv()` function in pandas to read in a file as a pandas dataframe (remember: our input is TAB separated, not comma separated!)\n",
    ">2. re-name the 'target_id' column to 'transcriptID' and the 'tpm' column to the `sampleID` (use the `.rename()` method of your dataframe [link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html))\n",
    ">3. subsets the data frame to only keep the two renamed columns\n",
    ">4. return the simplified data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePandasDF(sampleID,samplePath):\n",
    "    #Your code goes here: \n",
    "    \n",
    "    #read the tab-separated file as a Pandas dataframe\n",
    "\n",
    "    \n",
    "    #renaming two columns for clarity/ease of use later\n",
    "\n",
    "    \n",
    "    #making a list of the columns we want to retain\n",
    "    \n",
    "    #subset the dataframe to only be the two desired columns\n",
    "\n",
    "    \n",
    "    #return the simplified data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code:\n",
    "sampleID = \"AC_1873_20908\"\n",
    "filePath = \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/A01v1_20908_bl_S1_L003_abundance.tsv\"\n",
    "\n",
    "makePandasDF(sampleID, filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's expand our code further. We have five samples that we want to read in and convert to Pandas dataframes. We are going to create a dictionary where the `key` is the unique sample ID for each sample, and the `value` is the tab-separated list (i.e. the data) for each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have 5 samples and corresponding datasets\n",
    "#We will save them as a dictionary where the key = sample ID and value = path to the dataset\n",
    "samplesDict = {\n",
    "    \"AC_1873_20908\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/A01v1_20908_bl_S1_L003_abundance.tsv\",\n",
    "    \"AW_366493\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/B03v1_366493_h_S18_L003_abundance.tsv\",\n",
    "    \"AW_366490\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/E06v1_366490_g_S45_L003_abundance.tsv\",\n",
    "    \"AW_366494\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/E07v1_366494_br_S53_L003_abundance.tsv\",\n",
    "    \"AW_365335\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/E08v1_365335_e_S61_L003_abundance.tsv\"\n",
    "}\n",
    "\n",
    "print(samplesDict)\n",
    "print(samplesDict[\"AW_366494\"])\n",
    "example = samplesDict[\"AW_366494\"]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Excercise: write a loop that takes each sample and converts it to a pandas dataframe, using the function we created above. Hold all the pandas dataframes in a list. Remember: python dictionary keys are iterable, just like a list is. The result of your loop should be a list holding all the pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasDFs = [] #initialize a list to hold the pandas dataframes\n",
    "\n",
    "# Your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our finalized dataset, we will need to merge all the pandas dataframes together. Let's take a look at our dataframes to remind ourselves what they look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we tell head() to only print the first 5 lines, rather than the default 10\n",
    "pandasDFs[0].head(5)\n",
    "pandasDFs[1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When merging, we don't just want to concatenate all the data frames together; instead, we want to join the dataframes based on the values in the 'transcriptID' column. For this we will use the `pd.merge()` function in pandas, which combines two dataframes based on a shared column (or index). \n",
    "Let's look at a simple example first to see how exactly the `merge()` function works. Here we manually define two simple pandas dataframe using the `pd.DataFrame()` function, where `df1` has two columns (called 'A' and 'B') and `df2` has two columns (called 'A' and 'C') with one more row than `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "df2 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3', 'A4'],'C': ['C0','C1', 'C2', 'C3', 'C4']})\n",
    "\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we merge the two dataframes using the `pd.merge()` function, we can specify that the column to merge on is \"A\" and whether to keep only rows in common (default) or to keep all rows (`how='outer'`). Note that when we do the outer merge, the missing values are filled in with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.merge(df1, df2, on = 'A'))\n",
    "print(pd.merge(df1, df2, on = 'A', how = 'outer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `how = 'outer'` argument refers to the **type of join** we want to perform...the types of joins can get a little complicated and is beyond the scope of this class, but if you are familiar with `SQL` or the `tidyverse` package in `R`, it is basically identical to the joins in those languages. You can also check out the pandas merge documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#merge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Exercise: write a loop that merges our five pandas dataframes that we created above into a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF = pandasDFs[0] #we first need to an initial dataframe which we can then sequentially merge on\n",
    "\n",
    "#your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the final step, getting the gene name for each transcript! Recall that this information lives in the file `data/geneIDs.tsv`, so we are first going to have to read in this file as well. There are several ways we could do this, depending on what our specific goals are, but as we only want to retain the transcripts that have a corresponding gene ID in the table, the easiest way is again using the `merge()` function.\n",
    "\n",
    "Here's a sample of what the file looks like:\n",
    "\n",
    "```\n",
    "TCONS_00000001\tgene-SNX18_gGal-2\n",
    "TCONS_00000002\tgene-LOC116806879_tGut-like-2\n",
    "TCONS_00000004\tgene-KCMF1_gGal-like-1\n",
    "TCONS_00000003\tgene-KCMF1_gGal-like-1\n",
    "```\n",
    "\n",
    ">Exercise: read in the file `data/geneIDs.tsv` as a pandas dataframe and merge it with our combined dataframe to assign a gene name to each transcript \n",
    ">\n",
    "> Note that this import requires customizing the `pd.read_csv()` function because there is no header. You'll have to look at the documentation to figure out how to add a header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here:\n",
    "filename = \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Pythn/data/geneIDs.tsv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
