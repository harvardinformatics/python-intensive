{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Progamming Skills and Debugging\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to our last day of our python crash course. Today we will be going over some meta-programming skills that will be helpful as you continue to use python in your courses and research. We'll talk about how to read documentation and how to debug/troubleshoot. And then we'll put it to practice by working through more complex exercises. At the end of this session we will have time for freeform questions or to discuss other topics in python that you are interested in. As always, load the libraries by running the code block below to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to read documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming effectively actually involves a lot of reading** \n",
    "\n",
    "Primarily documentation, but also code, google results, stackexchange queries, etc. These are just a few examples of what you'll read as you work on code. Reading the documentation of a package or library or software that you are using should probably be the first thing you do when you start using it. However, software docs pages are a much different sort of writing than we may be used to, if we're primarily used to reading journal articles, textbooks, and protocols. Knowing how and how much to read documentation is a skill that needs to be developed over time to suit your own needs. There's definitely no need to read every single page of documentation of a piece of software, especially for large libraries like `numpy` or `matplotlib`. \n",
    "\n",
    "**There are a variety of ways software can be documented** \n",
    "\n",
    "You may be handed a single script from a coleague to perform some action and that script may have **comments** in the code detailing what it does or what certain lines do. Individual functions may have what is called a **docstring**, which is a string that occurs immediately after the function definition detailing how do use that function, inputs, and outputs. Another type of documentation is a docs page or **API reference** on a website for that software, such as the page for the seaborn's [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html) function. Many software packages also have some introductory pages like **vignettes** or **tutorials** that guide you through the basics of the software. The [Getting started tutorials](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html) of Pandas is a good example of this. \n",
    "\n",
    "**What documentation are we meant to read?** \n",
    "\n",
    "In general, documentation is meant to be a reference manual more than a textbook. A lot of documentation is really repetitive, because it has to exhaustively cover every single function and class available to the user. I do not recommend reading documentation like a book or in any linear way. That's like learning a foreign language by reding the dictionary. For example, `numpy` has a variety of [mathematical functions](https://numpy.org/doc/stable/reference/routines.math.html), but you are not required to look at the doc page of each of those. It is enough to know that it exists and when you do want to use a particular one, to check the page of that specific function. The most important parts of the documentation to read first are the tutorials/user guides, which introduce the basic functionality of the software with some example code. Often times, this code is exactly what you need to get started. If you get stuck, then it's time to read the docs pages for the specific commands you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a docs page\n",
    "\n",
    "Scientific articles typically have the same sections: Introduction, Methods, Results/Discussion. Similarly, docs pages for a function should all have some common components:\n",
    "\n",
    "* Function name and how to call it\n",
    "    * parameters in parentheses with any defaults showing\n",
    "    * positional parameters first, keyword parameters after asterisk\n",
    "* Description of function\n",
    "* Detailed parameters that can be passed to each function\n",
    "    * type of object that can be passed\n",
    "    * description of what the parameter does\n",
    "* Returns\n",
    "    * type of object(s) returned\n",
    "    * description of the object\n",
    "* Examples\n",
    "\n",
    "**Just the basics**\n",
    "\n",
    "If this is your first time encountering the function, glance at the function name and description and then go directly to the examples. This will help you understand if this function does what you think it does and give you a template to use it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Read the documentation page for plotting pie charts in matplotlib [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.pie.html). What is the minimal information you need to pass to the function to get a pie chart? What parameters are optional or already have defaults?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Troubleshooting**\n",
    "\n",
    "Looking at a docs page is helpful for troubleshooting certain errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Below is some code that is meant to plot a pie chart. However, it's not working. Can you figure out what's wrong? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Python', 'Java', 'C++', 'Ruby']\n",
    "sizes = [215, 130, 245, 210]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.pie(sizes, labs=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "# Aspect ratio to ensure the pie chart is circular.\n",
    "ax.axis('equal')  \n",
    "\n",
    "ax.set_title('Programming Language Popularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploring**\n",
    "\n",
    "If you are trying to find a specific way to customize the pie chart, it is worth reading the entire list of parameters to see what options are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Discussion:** Read the pie plot function's [documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.pie.html) page. How does the function give you control over the look of your pie wedges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Discussion:** What questions do you still have about the documentation page or about the pie function after reading it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to read code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to read other people's code is an important skill. Just like writing, everybody has their own coding style and when we learn to read and edit other people's code, it's mutually beneficial. Some things I've learned from reading other's code:\n",
    "\n",
    "* More efficient ways to do things\n",
    "* Common mistakes to avoid\n",
    "* New error messages and bugs\n",
    "* New problem solving strategies\n",
    "* How to write more readable/reproducible code\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a python script, reproduced from FASRC's [User Codes GitHub repo](https://github.com/fasrc/User_Codes), which is an underrated resource for anyone looking to run things on the Cannon high performance computer cluster. The script is called mc_pi.py and it calculates the value of pi via the monte-carlo method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Program: mc_pi.py\n",
    "         Monte-Carlo calculation of PI\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "N = 100000\n",
    "\n",
    "pi = 3.1415926535897932\n",
    "\n",
    "count = 0\n",
    "for i in range(N):\n",
    "    x = random.random()\n",
    "    y = random.random()\n",
    "    z = x*x + y*y\n",
    "    if z <= 1.0:\n",
    "        count = count + 1\n",
    "\n",
    "PI = 4.0*count/N\n",
    "\n",
    "print( \"Exact value of PI: {0:7.5f}\".format(pi) )\n",
    "print( \"Estimate of PI: {0:7.5f}\".format(PI) )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Discussion:** Can you describe in words what this script is doing? What extra information do you need to understand this script? **Note:** if the math behind this algorithm is not clear to you, you can find some explanations if you search for something like \"MCMC algorithm for estimating pi.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** The above python code uses a for loop to iterate over 100,000 values. This is rather slow. Rewrite this code to use numpy's [random](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html) function and take advantage of speedy array operations. If possible, make this into a function called `mc_pi` that takes as argument N, the number of simulations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "mc_pi(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Here is another example of someone elses's code that plots Boston's weather data. Can you annotate each chunk to describe what it does? Feel free to remove or adjust lines of the code and replay the block to see what each line does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/boston_weather_data.csv')\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df['time'].dt.year\n",
    "df['month'] = df['time'].dt.month\n",
    "df['month_name'] = df['time'].dt.strftime('%b')\n",
    "\n",
    "df_monthly_mean = df[['year', 'month', 'month_name', 'tavg', 'tmin', 'tmax']].groupby(['month', 'month_name']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "df_long = df_monthly_mean.melt(id_vars=['month', 'month_name'], value_vars=['tmin','tmax'], var_name='temperature_type', value_name='Temperature')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "tmin_data = df_long[df_long['temperature_type'] == 'tmin']\n",
    "tmax_data = df_long[df_long['temperature_type'] == 'tmax']\n",
    "\n",
    "ax.bar(tmin_data['month'] - 0.2, tmin_data['Temperature'], width=0.4, label='Min Temperature')\n",
    "ax.bar(tmax_data['month'] + 0.2, tmax_data['Temperature'], width=0.4, label='Max Temperature')\n",
    "\n",
    "ax.plot(df_monthly_mean['month'], df_monthly_mean['tavg'], marker='o', label='Avg Temperature', color='black')\n",
    "\n",
    "ax.set_xticks(df_monthly_mean['month'])\n",
    "ax.set_xticklabels(df_monthly_mean['month_name'])\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Temperature (Celsius)\")\n",
    "ax.set_title(\"Boston 2013-2023 Temperatures\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Discussion:** What did you learn about pandas or plotting from reading this code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to get unstuck: troubleshooting/debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to be talking about debugging code. We typically picture debugging as something that happens when you run something and there's an error message. However, there are many other reasons why we might want to take a closer look at our code and the tips here will be useful throughout the code-writing process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual debugging\n",
    "\n",
    "Here are some steps to figure out what's wrong that just involve using your own brain. This is usually my first resort as it is quick and many issues end up being about a simple typo or missing step. \n",
    "\n",
    "* Read the error message\n",
    "    * What line does it refer to?\n",
    "    * What state should the code be in at that point?\n",
    "* (Re)Read your code\n",
    "    * Explain it line by line to another person or an inanimate object\n",
    "* Add error checks\n",
    "    * Print messages to check variables and progress\n",
    "    * Peel back layers and test each layer (e.g. test each function)\n",
    "* Get another pair of eyes to look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "A very helpful approach to debugging - and, indeed, writing code in general - is to have some small input data that you can use to test. Ideally this should be something that runs fast and where you know what the expected output is. While this won't necessarily help you fix problems that cause your code to simply not execute, it can be extremely useful if you are not certain of how to program something. \n",
    "\n",
    "For example, if you have a dataset that you have manually processed, but now want to automate with a Python script or set of scripts, you will very likely want to check your code by running it against a test input that has a known output. \n",
    "\n",
    "This is particularly important if you are going to use LLMs for debugging, as discussed below. LLMs rarely make syntax errors, but do make logic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the internet\n",
    "\n",
    "StackExchange-type websites are great resources for learning about the code or software you are running. These answers are what the LLMs are trained on so while it may be less convenient to read through a stackexchange answer, you will get more correct information and also more context than asking an LLM the same thing. You'll probably land on a stackexchange website if you've googled an error message or the name of your software plus some issue. Lots of people have asked lots of questions over the years and you can often find someone who has had the same problem as you.\n",
    "\n",
    "Even if you don't find the exact answer to your question, it's helpful to read about and participate in the community of people who use the same software as you. So don't be shy about posting something if you need specific advice. I've found that the Biostars community is fairly friendly.\n",
    "\n",
    "Here are some examples of questions on Biostars that I found were more helpful or interesting than what an LLM might produce:\n",
    "\n",
    "* [How To Efficiently Parse A Huge Fastq File?](https://www.biostars.org/p/10353/)\n",
    "* [Mean Length of Fasta Sequences](https://www.biostars.org/p/1758/)\n",
    "* [Why Don't We Use Binary Format [For storing DNA data]?](https://www.biostars.org/p/75178/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking for help\n",
    "\n",
    "Often times, the quickest way to get unstuck is to ask someone for help. There are some steps you can take to make it easier for others to help you. You may know all the context of your code, but a friend or one of us at office hours is going in fresh. Here's some information that you should provide when asking for help (in approximate order of effort):\n",
    "\n",
    "1. Error message\n",
    "2. What you expected to happen\n",
    "3. What is the command you used\n",
    "4. Your environment/context\n",
    "5. A minimal reproducible example\n",
    "\n",
    "Numbers 1 through 3 are the bare minimum information, while 4 and 5 are helpful for trickier problems. Number 5 is especially important if you are asking for help in an asynchronous way, like on a forum or in an email. This allows the person helping you to run code to see the error message for themselves and test out solutions before getting back to you. \n",
    "\n",
    "If you're not familiar with **minimal reproducible examples**, it's a way to pare down your code to the smallest amount that still produces the error. Often in the process of doing this, you will find the error yourself. How to make a reproducible example (AKA **reprex**)? Here are some steps:\n",
    "\n",
    "1. Start with a fresh script\n",
    "2. Import only the libraries you need\n",
    "3. Create only the data objects/variables you need (You may need to generate data or subset your data if it's large)\n",
    "4. Write only the code that produces the error\n",
    "5. Annotate the code with comments to explain what you are trying to do\n",
    "\n",
    "If you want to then share this code (and the dummy data!) with someone else, you can either send the script to them or you can use a python package called `reprexpy`, which will format both your code and your output in a way that is easy to post in plain text online or in an email. For more information see the docs for [reprexpy](https://reprexpy.readthedocs.io/en/latest/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLMs to debug code\n",
    "\n",
    "Here are some ways you can use LLMs to fix code:\n",
    "\n",
    "* Use a chat-based LLM\n",
    "* Use GitHub Copilot plugin for VSCode\n",
    "\n",
    "Using LLMs to fix code is similar to asking a person for help: you want to include the context of the error as much as possible. If your LLM is integrated into your code editor, it can be as simple as highlighting the section and telling it what you expect the code to do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error messages (also known as exceptions)\n",
    "\n",
    "In python, there are a few types of error messages that you might encounter. Here are some common ones:\n",
    "\n",
    "- `SyntaxError`: You have a typo like a missing parenthesis or bracket\n",
    "- `NameError`: You are trying to use a variable that hasn't been defined\n",
    "- `TypeError`: You are trying to use a variable in a way that is not allowed\n",
    "- `ValueError`: You are trying to use a variable with a value that is not allowed\n",
    "- `IndexError`: You are trying to access an index that doesn't exist\n",
    "- `KeyError`: You are trying to access a key that doesn't exist\n",
    "- `AttributeError`: You are trying to access an attribute that doesn't exist\n",
    "\n",
    "Look for these keywords in the error message to help you figure out what's wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exception Handling\n",
    "\n",
    "Exception handling is a fundamental concept in Python that allows you to manage and respond to errors gracefully, ensuring that a program can continue running or fail in a controlled manner. When your code encounters an issue—such as trying to divide by zero, accessing a file that doesn't exist, or converting user input to the wrong type—Python raises an \"exception.\" Without proper handling, these exceptions can (and usually do) cause your program to crash, or worse create unpredictable bugs. By using try and except blocks, you can anticipate potential errors, catch these exceptions, and define alternative actions or informative messages for the user. \n",
    "\n",
    "As an example, here is a simple function that calcutes the log2 fold change, that is log2(x/y). We will write this naively, and then see what errors we get with different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log2fc(x, y):\n",
    "    return math.log2(x/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2fc(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2fc(10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2fc(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log2fc(\"a\", \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three kinds of errors here, that we might want to handle. The first is a \"ZeroDivisionError\", when we try to divide by 0. The second is a ValueError, since the math.log2 function does not like returning -Inf for log2(0), and in our function we probably don't want that either. The last is a TypeError if we give inputs that don't have division methods (e.g., strings).\n",
    "\n",
    "For any error, we can \"catch\" the error with a try-except statement, which has the following basic form:\n",
    "\n",
    "```\n",
    "try:\n",
    "    code to try goes here\n",
    "except ErrorType:\n",
    "    code to run if try block generates ErrorType goes here\n",
    "```\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- Specifying Error Types: You don't need to specify an error type; if you don't, the except block will catch any error. Normally, this isn't a good idea because you might want to return specific messages or handle particular error types differently.\n",
    "- Multiple Except Blocks: except statements function somewhat like elif: you can have many of them, each with a different ErrorType. You can also list multiple error types within a single except statement using parentheses, e.g., `except (KeyError, ValueError):` if you want to catch both errors.\n",
    "- Else and Finally Blocks: You can have an `else` block (which runs only if no errors occur) or a `finally` block (which runs after the try-except regardless of whether an error occurred)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a slightly improved version of our function, that catches TypeErrors. Can you modify this to also catch ZeroDivisionError? If we get that error, let's adjust the denominator to y = y + (x/10) and recalculate. For a bigger challenge, think about how you might also catch ValueErrors. Note that these are coming from the `math.log2(ratio)` not `ratio = x/y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2fc(x, y):\n",
    "    try:\n",
    "        ratio = x/y\n",
    "    except TypeError:\n",
    "        print(\"Error: x and y must be numbers\")\n",
    "        return None\n",
    "    \n",
    "    return math.log2(ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Modify the above function to also catch ZeroDivisionError and ValueError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** write a function that takes a DataFrame and a column, and returns the average of that column. If you can't take the mean of the column, print an informative error message and return `None`.\n",
    "The wine dataset is a good test case for this: use the following code to load the wine dataset as a pandas DataFrame, and also create a test dataset with errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.read_csv('https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/winequality-red.csv', delimiter=';')\n",
    "wines_error = wines.copy()\n",
    "# Make a few columns be non-numeric\n",
    "wines_error['alcohol'] = np.random.choice(['high', 'medium', 'low'], len(wines))\n",
    "wines_error['quality'] = np.random.choice(['excellent', 'good', 'meh', 'bad'], len(wines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def analyze_wines(df, column): # keep\n",
    "    \"\"\"Compute and print the mean of a DataFrame column.\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code here\n",
    "analyze_wines(wines, 'alcohol')\n",
    "analyze_wines(wines_error, 'alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iNaturalist Data exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise will work with the iNaturalist bird data from 2023 Massachusetts. The individual bird observations are stored in a file called `bird_observations.csv` and the mapping of bird taxon ids to common names is stored in a file called `bird_names.csv`. We've previously imported both as dictionaries and recorded the number of observations of each bird. \n",
    "\n",
    "This time, we will ask you to **find the most common bird sighting for each month and print a dataframe with the month, the bird name, and the number of sightings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some handy functions/methods you might find useful. You may or may not use all of these. \n",
    "\n",
    "| Function | Description |\n",
    "| --- | --- |\n",
    "| `pd.read_csv()` | Read a csv file into a DataFrame |\n",
    "| `pd.merge()` | Merge two DataFrames together |\n",
    "| `pd.to_datetime()` | Convert a column to datetime |\n",
    "| `.dt.month` | Extract the month from a datetime column |\n",
    "| `.groupby()` | Group the DataFrame by a column |\n",
    "| `.size()` | Count the number of observations in each group |\n",
    "| `.transform()` | Perform group-wise calculations while preserving DataFrame shape |\n",
    "| `.loc[]` | Select rows and columns by index |\n",
    "| `.reset_index()` | Reset the index of a DataFrame (make the index into a column)|\n",
    "| `.merge()` | Merge two DataFrames together |\n",
    "| `.query()` | Query the columns of a DataFrame with a boolean expression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/bird_names.csv'\n",
    "\n",
    "# Your code here to import bird_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/bird_observations.csv'\n",
    "\n",
    "# Your code here to import bird_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to merge the two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to find the most common bird species for each month\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating summary statistics and correlations of the RNA-seq data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will create a dataframe that contains the mean and standard deviation of all the genes in the RNA-seq dataset. We will also calculate the correlation between the TPM values of each sample. First, here is the code to load the data into a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesDict = {\n",
    "    \"AC_1873_20908\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/A01v1_20908_bl_S1_L003_abundance.tsv\",\n",
    "    \"AW_366493\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/B03v1_366493_h_S18_L003_abundance.tsv\",\n",
    "    \"AW_366490\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E06v1_366490_g_S45_L003_abundance.tsv\",\n",
    "    \"AW_366494\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E07v1_366494_br_S53_L003_abundance.tsv\",\n",
    "    \"AW_365335\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E08v1_365335_e_S61_L003_abundance.tsv\"\n",
    "}\n",
    "\n",
    "def makePandasDF(sampleID,samplePath):\n",
    "    df = pd.read_csv(samplePath, sep='\\t') #read the tab-separated file as a Pandas dataframe\n",
    "    df = df.rename(columns={'target_id': 'transcriptID', 'tpm': f\"{sampleID}_tpm\"}) #renaming two columns for clarity/ease of use later\n",
    "    keepColumns = ['transcriptID', f\"{sampleID}_tpm\"] #making a list of the columns we want to retain\n",
    "    dfSimple = df[keepColumns] #subset the dataframe to only be the two desired columns\n",
    "    return dfSimple #return the simplified data frame\n",
    "\n",
    "pandasDFs = []\n",
    "\n",
    "for sample in samplesDict:\n",
    "    filePath = samplesDict[sample]\n",
    "    pandasDFs.append(makePandasDF(sample,filePath))\n",
    "\n",
    "# making the base dataframe to merge onto\n",
    "mergedDF = pandasDFs[0]\n",
    "\n",
    "for df in pandasDFs[1:]:\n",
    "    mergedDF = pd.merge(mergedDF, df, on='transcriptID')\n",
    "\n",
    "gene_df = pd.read_csv(\"https://informatics.fas.harvard.edu/resources/Workshops/Python/data/geneIDs.tsv\", sep='\\t', header=None, names=['transcriptID', 'geneID'])\n",
    "mergedDF = pd.merge(mergedDF, gene_df, on='transcriptID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final dataframe that summarizes expression information for our 5 samples, we can start to pull useful information out of it! Let's consider the sorts of information we could want from such a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** For each sample in our dataset, we want to calculate the mean expression across all transcripts...in other words, we want the mean of each of our sample columns. Make sure you are correctly using the index to pull out the desired columns, then use the `.mean()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Exercise: For each transcript (i.e. each row), we want to calculate the mean expression and the standard deviation, and add those both as columns to your data frame. In other words, we want to end up with something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| transID   | sample1_TPM | sample2_TPM | sample3_TPM | sample4_TPM | sample5_TPM | geneID | meanTPM | meanSTD |\n",
    "|-----------|-------------|-------------|-------------|-------------|-------------|--------|---------|---------|\n",
    "| TCONS_01  | 0           | 0           | 1.2         | 5           | 3           | gene1  | x       | x       |\n",
    "| TCONS_02  | 21.4        | 12.3        | 10.0        | 0           | 9.3         | gene1  | y       | y       |\n",
    "| TCONS_03  | 3.3         | 0.12        | 0           | 0           | 4.7         | gene2  | z       | z       |\n",
    "| Etc. etc. |             |             |             |             |             |        |         |         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to consider:\n",
    "- As before, we don't want the ENTIRE row (as certain columns contain non-numerical info, e.g. gene name), just the range of columns with TPM info \n",
    "- Remember numpy has handy built-in function for mean and STD...also remember the useful 'axis' argument\n",
    "- To add a new column to a pandas dataframe, the syntax is quite simple: `df['name_for_new_column'] = some_1D_array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping data in a pandas dataframe\n",
    "So far, when summarizing the data in our dataframe we've considered the most basic instances where we want to group down columns or across rows. However, we can use pandas to define our own groups and do more complex analysis!\n",
    "First, let's do a quick test of our understanding to make sure we understand our data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the transcript IDs have unique values, but there are duplicates in the gene ID column; why is this? Is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">! Answer: no, it is expected! Genes can have *multiple transcripts*, i.e. genes have multiple isoforms. So while each transcript ID should be unique, it is totally fine to have dupliucate gene IDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO: let's say that we now want information on a *per-gene* basis. For example, say we want to know how many isoforms each gene has. To accomplish this, we can use pandas' `groupby()` function, which splits a dataframe and groups it based on a *specified label*. In our case, we want to group based on gene ID, then pair it with the `size()` function to count how large each group is, i.e. how many isoforms a gene has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.groupby('geneID').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a 1D pandas Series, which you can convert back to a dataframe by using the `.reset_index()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.groupby('geneID').size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Add a column to the `mergedDF` dataframe that has the number of isoforms for the given gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPMs or transcripts per million are a useful way for summarizing expression levels for features of interest, whether they be of alternative spliced transcripts of a gene (i.e. isoforms) or the aggregated gene-level expression summed across all annotated isoforms. Kallisto generates isoform-level expression estimates, but it is often of interest to calculate gene-level expression. For a study of scrubjays gene expression, we have tables of isoform-level expression in the form of TPM for several samples. We also have a table that indicates which gene an isoform originates from. We've already merged those together as `mergedDF`. \n",
    "\n",
    "Use pandas `groupby`, `sum`, and perhaps other functions to create a single table where there is a column for gene id, and a separate column for each sample that represents the sum of TPM across all isoforms of a gene. Then, calculate all pairwise correlation coefficients (`.corr()` on your dataframe) between the tpm columns to see how strongly pairs of samples are correlated in their expression levels across genes.\n",
    "\n",
    "You should end up with a table that looks like this:\n",
    "\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>AC_1873_20908_tpm</th>\n",
    "      <th>AW_366493_tpm</th>\n",
    "      <th>AW_366490_tpm</th>\n",
    "      <th>AW_366494_tpm</th>\n",
    "      <th>AW_365335_tpm</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>AC_1873_20908_tpm</th>\n",
    "      <td>1.000000</td>\n",
    "      <td>0.040025</td>\n",
    "      <td>0.394478</td>\n",
    "      <td>0.040464</td>\n",
    "      <td>0.093961</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>AW_366493_tpm</th>\n",
    "      <td>0.040025</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>-0.111748</td>\n",
    "      <td>0.821287</td>\n",
    "      <td>0.825275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>AW_366490_tpm</th>\n",
    "      <td>0.394478</td>\n",
    "      <td>-0.111748</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>-0.082210</td>\n",
    "      <td>-0.080825</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>AW_366494_tpm</th>\n",
    "      <td>0.040464</td>\n",
    "      <td>0.821287</td>\n",
    "      <td>-0.082210</td>\n",
    "      <td>1.000000</td>\n",
    "      <td>0.931355</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>AW_365335_tpm</th>\n",
    "      <td>0.093961</td>\n",
    "      <td>0.825275</td>\n",
    "      <td>-0.080825</td>\n",
    "      <td>0.931355</td>\n",
    "      <td>1.000000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a function to calculate the logfold change of the RNA-seq dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will be working with the kallisto output files. We will write a function that takes as input a gene symbol, like `KCMF1`, and optionally an individual. We will then do the following:\n",
    "- Extract all rows of the kallisto DataFrame where the gene columns contain the gene symbol text\n",
    "- Group the DataFrame by the exact text in the geneID column\n",
    "- Sum the expression across transcripts to create a gene expression level\n",
    "- Calculate the mean of the gene expression for AW individuals\n",
    "- Calculate the log2 fold change with respect to the specified individual (e.g., log2(mean AW / AC))\n",
    "- For each gene that matches that symbol, print out the geneID, the mean, and the log2 fold change.\n",
    "\n",
    "The output should be something like:\n",
    "```\n",
    "geneID: gene-KCMF1_gGal-like-1; mean AW exp: 25.2; log2 fold change (mean AW / {input_individual}): 100.3\n",
    "```\n",
    "\n",
    "You can be as fancy as you like, but make sure to do a few things:\n",
    "- If the gene symbol is not found in the data frame, make sure to catch that error and report \"No transcripts matching {input_gene} found!\" or something similar.\n",
    "- If the individual is not found in the data frame, catch that error and report \"Invalid reference individual {ref_individual}\" or something similar.\n",
    "- Catch numerical errors in the log2 calculations\n",
    "\n",
    "In some cases you might want to raise specific errors, e.g. you can use: \n",
    "```python\n",
    "if matching_rows.empty:\n",
    "    raise ValueError(f\"No transcripts matching {gene_symbol} found!\")\n",
    "```\n",
    "to raise your own ValueError when you don't find any transcripts matching the gene symbol (although, of course, make sure to do this in a try-except block where the ValueError can be caught!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's re-run the code to get the `mergedDF` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesDict = {\n",
    "    \"AC_1873_20908\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/A01v1_20908_bl_S1_L003_abundance.tsv\",\n",
    "    \"AW_366493\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/B03v1_366493_h_S18_L003_abundance.tsv\",\n",
    "    \"AW_366490\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E06v1_366490_g_S45_L003_abundance.tsv\",\n",
    "    \"AW_366494\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E07v1_366494_br_S53_L003_abundance.tsv\",\n",
    "    \"AW_365335\": \"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/E08v1_365335_e_S61_L003_abundance.tsv\"\n",
    "}\n",
    "\n",
    "def makePandasDF(sampleID,samplePath):\n",
    "    df = pd.read_csv(samplePath, sep='\\t') #read the tab-separated file as a Pandas dataframe\n",
    "    df = df.rename(columns={'target_id': 'transcriptID', 'tpm': f\"{sampleID}_tpm\"}) #renaming two columns for clarity/ease of use later\n",
    "    keepColumns = ['transcriptID', f\"{sampleID}_tpm\"] #making a list of the columns we want to retain\n",
    "    dfSimple = df[keepColumns] #subset the dataframe to only be the two desired columns\n",
    "    return dfSimple #return the simplified data frame\n",
    "\n",
    "pandasDFs = []\n",
    "\n",
    "for sample in samplesDict:\n",
    "    filePath = samplesDict[sample]\n",
    "    pandasDFs.append(makePandasDF(sample,filePath))\n",
    "\n",
    "# making the base dataframe to merge onto\n",
    "mergedDF = pandasDFs[0]\n",
    "\n",
    "for df in pandasDFs[1:]:\n",
    "    mergedDF = pd.merge(mergedDF, df, on='transcriptID')\n",
    "\n",
    "gene_df = pd.read_csv(\"https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/geneIDs.tsv\", sep='\\t', header=None, names=['transcriptID', 'geneID'])\n",
    "mergedDF = pd.merge(mergedDF, gene_df, on='transcriptID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** To get started, first write a function to extract all rows of the kallisto dataframe where the gene columns contain a specific string. If there are no matching rows, print \"No transcripts matching {input_gene} found!\".\n",
    "\n",
    "You probably want to look at the `.str.contains()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "def analyze_gene_expression(df, gene_symbol):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "analyze_gene_expression(mergedDF, 'HINT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Next, modify the funtion so that it groups the dataframe by the exact text in the geneID column and  calculates the mean expression across all AW individuals (columns with 'AW' in the name). You will first want to find the sum of expression of all transcripts for each AW individual, and then take the mean of that. Your output should be a printing out a string that says \"geneID: {geneID}; mean AW exp: {mean_exp}\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gene_expression(df, gene_symbol):\n",
    "    \n",
    "    # Step 1: Filter rows where 'Gene ID' contains the gene_symbol (case-insensitive)\n",
    "    \n",
    "\n",
    "    # Step 2: Identify sample columns (excluding 'Gene ID' and 'Transcript ID')\n",
    "    \n",
    "    # Step 3: Group by 'Gene ID' and sum expression across transcripts\n",
    "\n",
    "    \n",
    "    # Step 4: Identify 'AW' columns (assuming they start with 'AW')\n",
    "    \n",
    "    # Step 5: Calculate mean expression for 'AW' individuals\n",
    "    grouped['Mean_AW'] = grouped[aw_columns].mean(axis=1)\n",
    "\n",
    "    # Step 6: Print all the gene IDs and mean expression for 'AW' individuals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_gene_expression(mergedDF, 'HINT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Now we will add a new functionality to our function. We will calculate the mean expression of the gene for a specific individual \"AC_1873_20908_tpm\" and calculate the log2 fold change with respect to that individual. \n",
    ">\n",
    "> First, we add a new argument `individual=None` to the function definition. You will then write an if else statement so that the default behavior is to return what we've already done, but if the individual is supplied, we will return the expression levels of that individual instead. If the individual is not found in the data frame, catch that error and report \"Invalid reference individual {ref_individual}\" or something similar.\n",
    ">\n",
    "> (we will worry about the log2 fold change in the next exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gene_expression(df, gene_symbol, individual=None):\n",
    "\n",
    "    # Step 1: Filter rows where 'Gene ID' contains the gene_symbol (case-insensitive)\n",
    "    \n",
    "\n",
    "    # Step 2: Identify sample columns (excluding 'Gene ID' and 'Transcript ID')\n",
    "    \n",
    "    \n",
    "    # Step 3: Group by 'Gene ID' and sum expression across transcripts\n",
    "    \n",
    "    \n",
    "    # Step 4: Identify 'AW' columns (assuming they start with 'AW')\n",
    "    \n",
    "    \n",
    "    # Step 5: Calculate mean expression for 'AW' individuals\n",
    "    \n",
    "    # If individual is specified, calculate mean expression of just that individual and print it out\n",
    "\n",
    "    # else, calculate mean expression for 'AW' individuals and print it out as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_gene_expression(mergedDF, 'HINT1', \"AC_1873_20908_tpm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_gene_expression(mergedDF, 'HINT1', \"wrong_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Now let's make a function (within our analyze_gene_expression function) that calculates the log2 fold change of the AW gene expressions with respect to the individual. Remember our earlier lesson of try-except blocks to catch situtations where the denominator is 0 and other potential errors. It is a good idea to build that into our little sub function. \n",
    ">\n",
    "> A note when writing this function: the log2 fold change is calculated as log2(mean AW / {input_individual}).\n",
    ">\n",
    "> Also, when writing custom helper functions within a function, you can use any of the variables that are defined in the outer function.\n",
    ">\n",
    "> Once you have that sub-function, you can call it within the main function and `apply` it to each row of your dataframe that contains all the average expression levels of the genes.\n",
    ">\n",
    "> Now, if the individual is specified, the `analyze_gene_expression` function should print out the geneID, the mean expression of the gene for the AW individuals, and the log2 fold change with respect to the individual.\n",
    ">\n",
    "> If the individual is not specified, the function should print out the geneID and the mean expression of the gene for the AW individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gene_expression(df, gene_symbol, individual=None):\n",
    "    # Step 1: Filter rows where 'Gene ID' contains the gene_symbol (case-insensitive)\n",
    "    \n",
    "    \n",
    "    # Step 2: Identify sample columns (excluding 'Gene ID' and 'Transcript ID')\n",
    "\n",
    "    \n",
    "    # Step 3: Group by 'Gene ID' and sum expression across transcripts\n",
    "\n",
    "    \n",
    "    # Step 4: Identify 'AW' columns (assuming they start with 'AW')\n",
    "\n",
    "    \n",
    "    # Calculate mean expression for 'AW' individuals\n",
    "\n",
    "    \n",
    "    # If individual is specified, calculate mean expression and log2 fold change\n",
    "    \n",
    "        # Calculate mean expression for the specified individual\n",
    "        \n",
    "        # Calculate log2 fold change, handling numerical issues\n",
    "  \n",
    "        # Print results with log2 fold change\n",
    "\n",
    "\n",
    "    # Else if individual is not specified, print results without log2 fold change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_gene_expression(mergedDF, 'HINT1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_gene_expression(mergedDF, 'HINT1', \"AC_1873_20908_tpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Exercise:** Now that you've written a working function, it's important to add a \"docstring\" to the beginning of it. Use triple quotes just after your function definition to write a description of what the function does, what the input arguments are, and what the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy your function here and add a docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indiana storm data exercise\n",
    "\n",
    "Here is a dataset of storm reports in Indiana counties in 2015. We'll be using it to practice writing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line downloads the file locally to the same folder as your notebook\n",
    "!wget -P data https://informatics.fas.harvard.edu/resources/Workshops/2024-Fall/Python/data/Indiana_Storms.csv\n",
    "\n",
    "# This line stores the local file path as a Python string variable\n",
    "storms_file = 'data/Indiana_Storms.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a file that contains data from the National Weather Service. It contains data on storm events in Indiana in 2015. While there are 50 columns in the data, we are primarily interested in type of storm event \"EVENT_TYPE\" (13th column), the county in which the event occurred \"CZ_NAME\" (16th column), and the time of the event \"BEGIN_DATE_TIME\" (18th column).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the file into a dictionary using `csv.DictReader`, a DataFrame using `pandas.read_csv`, a nested list, or any other data structure. Consider the following and look ahead at the exercises to make the best decision you can about how to read this file.\n",
    "- What type of file is this and what methods can you use to read that file type?\n",
    "- Which data structure(s) makes most sense given the task?\n",
    "\n",
    "For reading into a list or csv.DictReader, you will use the `with open() as file:` syntax. For reading into a DataFrame, you will use the `pd.read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function that takes as input the storms object you created and a county name, and prints all of the events that occurred in a given county and the date they began. Include an optional argument `storm_type` that displays only certain storm types (hint: this means setting a default value to the argument that includes all storm types). Your function should print \"A {event_type} happened on {date} in {county} county.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your function here\n",
    "\n",
    "# def storm_by_county(INSERT ARGUMENTS):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Test your function by:\n",
    "- Printing all storm events in TIPPECANOE county\n",
    "- Printing all \"Flood\" and \"Flash Flood\" events in the following counties: MARION, MONROE, SPENCER, VERMILLION\n",
    "    - Try to make this loop through the list of counties\n",
    "- Printing all \"Tornado\" events in the state (You will need a extract a list of all counties from your data structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your storm_by_county function\n",
    "\n",
    "# 3.1: Display all storm events in TIPPECANOE county.\n",
    "print(\"--- 3.1 ---\")\n",
    "\n",
    "\n",
    "# 3.2: Display all Flood and Flash Flood events in the following counties: MARION, MONROE, SPENCER, VERMILLION\n",
    "print(\"--- 3.2 ---\")\n",
    "\n",
    "# 3.3: Display all Tornado events that occurred in the state.\n",
    "print(\"--- 3.3 ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Modify your function so that it also returns the NUMBER of events displayed. Next, add a boolean argument `display_events` that if False, supresses the printing of the events (only the count should be returned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your function here\n",
    "\n",
    "# def storm_by_county(INSERT ARGUMENTS):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test your function by:\n",
    "- Printing the number of all events in PIKE county without printing information about each one\n",
    "- Printing the total number of \"Thunderstorm Wind\" events in the following counties: ELKHART, LA PORTE, BOONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your modified storm_by_county function\n",
    "\n",
    "# 5.1: Display the number of all events in PIKE county without printing information about each one\n",
    "print(\"--- 5.1 ---\")\n",
    "\n",
    "\n",
    "# 5.2: Display only the TOTAL number of Thunderstorm Wind events in the following counties: ELKHART, LA PORTE, BOONE\n",
    "print(\"--- 5.2 ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Use your function to find the most common event in TIPPECANOE county and display the dates of those events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use your function to find and display the most common storm event in TIPPECANOE county\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
